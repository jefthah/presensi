{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e79abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 0us/step\n",
      "\n",
      "No existing model found or error loading model\n",
      "Found existing dataset, training new model...\n",
      "\n",
      "Loading dataset from dataset\n",
      "Processing NIM: 2110511131\n",
      "Processing NIM: 2110511150\n",
      "\n",
      "Dataset Summary:\n",
      "Total samples loaded: 50\n",
      "Total unique NIMs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_203         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_204         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m786,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_203         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_204         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">921,858</span> (3.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m921,858\u001b[0m (3.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">920,322</span> (3.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m920,322\u001b[0m (3.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training classifier model with FaceNet embeddings...\n",
      "Epoch 1/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281ms/step - accuracy: 0.5938 - loss: 10.6749 - val_accuracy: 0.9000 - val_loss: 9.7417 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 9.0983 - val_accuracy: 1.0000 - val_loss: 9.3550 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 8.7016 - val_accuracy: 1.0000 - val_loss: 8.9545 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 8.3155 - val_accuracy: 1.0000 - val_loss: 8.5536 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 7.9355 - val_accuracy: 1.0000 - val_loss: 8.1672 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.5574 - val_accuracy: 1.0000 - val_loss: 7.7989 - learning_rate: 0.0010\n",
      "Epoch 7/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 7.2058 - val_accuracy: 1.0000 - val_loss: 7.4495 - learning_rate: 0.0010\n",
      "Epoch 8/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 6.8650 - val_accuracy: 1.0000 - val_loss: 7.1212 - learning_rate: 0.0010\n",
      "Epoch 9/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 6.5363 - val_accuracy: 1.0000 - val_loss: 6.8074 - learning_rate: 0.0010\n",
      "Epoch 10/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 6.2288 - val_accuracy: 1.0000 - val_loss: 6.5078 - learning_rate: 0.0010\n",
      "Epoch 11/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.9346 - val_accuracy: 1.0000 - val_loss: 6.2221 - learning_rate: 0.0010\n",
      "Epoch 12/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 5.6543 - val_accuracy: 1.0000 - val_loss: 5.9504 - learning_rate: 0.0010\n",
      "Epoch 13/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 5.3863 - val_accuracy: 1.0000 - val_loss: 5.6921 - learning_rate: 0.0010\n",
      "Epoch 14/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 5.1333 - val_accuracy: 1.0000 - val_loss: 5.4477 - learning_rate: 0.0010\n",
      "Epoch 15/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9833 - loss: 4.9152 - val_accuracy: 1.0000 - val_loss: 5.2280 - learning_rate: 0.0010\n",
      "Epoch 16/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 4.6734 - val_accuracy: 0.9000 - val_loss: 5.0323 - learning_rate: 0.0010\n",
      "Epoch 17/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 4.4737 - val_accuracy: 0.9000 - val_loss: 4.8480 - learning_rate: 0.0010\n",
      "Epoch 18/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9833 - loss: 4.3390 - val_accuracy: 0.9000 - val_loss: 4.6795 - learning_rate: 0.0010\n",
      "Epoch 19/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.1151 - val_accuracy: 0.7000 - val_loss: 4.5288 - learning_rate: 0.0010\n",
      "Epoch 20/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.9598 - val_accuracy: 0.7000 - val_loss: 4.3883 - learning_rate: 0.0010\n",
      "Epoch 21/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.8136 - val_accuracy: 0.7000 - val_loss: 4.2549 - learning_rate: 0.0010\n",
      "Epoch 22/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 3.6751 - val_accuracy: 0.5000 - val_loss: 4.1278 - learning_rate: 0.0010\n",
      "Epoch 23/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 3.5450 - val_accuracy: 0.5000 - val_loss: 4.0060 - learning_rate: 0.0010\n",
      "Epoch 24/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.4314 - val_accuracy: 0.5000 - val_loss: 3.8880 - learning_rate: 0.0010\n",
      "Epoch 25/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 3.3040 - val_accuracy: 0.5000 - val_loss: 3.7760 - learning_rate: 0.0010\n",
      "Epoch 26/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 3.2002 - val_accuracy: 0.5000 - val_loss: 3.6705 - learning_rate: 0.0010\n",
      "Epoch 27/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 3.0862 - val_accuracy: 0.5000 - val_loss: 3.5702 - learning_rate: 0.0010\n",
      "Epoch 28/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.9863 - val_accuracy: 0.5000 - val_loss: 3.4764 - learning_rate: 0.0010\n",
      "Epoch 29/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.8969 - val_accuracy: 0.5000 - val_loss: 3.3845 - learning_rate: 0.0010\n",
      "Epoch 30/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.8016 - val_accuracy: 0.5000 - val_loss: 3.2959 - learning_rate: 0.0010\n",
      "Epoch 31/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.7126 - val_accuracy: 0.5000 - val_loss: 3.2142 - learning_rate: 0.0010\n",
      "Epoch 32/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.6288 - val_accuracy: 0.5000 - val_loss: 3.1355 - learning_rate: 0.0010\n",
      "Epoch 33/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 2.5505 - val_accuracy: 0.5000 - val_loss: 3.0614 - learning_rate: 0.0010\n",
      "Epoch 34/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 2.4711 - val_accuracy: 0.5000 - val_loss: 2.9931 - learning_rate: 0.0010\n",
      "Epoch 35/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.3973 - val_accuracy: 0.5000 - val_loss: 2.9297 - learning_rate: 0.0010\n",
      "Epoch 36/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.3259 - val_accuracy: 0.5000 - val_loss: 2.8687 - learning_rate: 0.0010\n",
      "Epoch 37/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 2.2585 - val_accuracy: 0.5000 - val_loss: 2.8071 - learning_rate: 0.0010\n",
      "Epoch 38/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 2.1918 - val_accuracy: 0.5000 - val_loss: 2.7484 - learning_rate: 0.0010\n",
      "Epoch 39/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 2.1280 - val_accuracy: 0.5000 - val_loss: 2.6902 - learning_rate: 0.0010\n",
      "Epoch 40/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 2.0665 - val_accuracy: 0.5000 - val_loss: 2.6357 - learning_rate: 0.0010\n",
      "Epoch 41/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.0080 - val_accuracy: 0.5000 - val_loss: 2.5808 - learning_rate: 0.0010\n",
      "Epoch 42/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9833 - loss: 1.9700 - val_accuracy: 0.5000 - val_loss: 2.5412 - learning_rate: 0.0010\n",
      "Epoch 43/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 1.8976 - val_accuracy: 0.5000 - val_loss: 2.5214 - learning_rate: 0.0010\n",
      "Epoch 44/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.8489 - val_accuracy: 0.5000 - val_loss: 2.5013 - learning_rate: 0.0010\n",
      "Epoch 45/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9833 - loss: 1.8380 - val_accuracy: 0.5000 - val_loss: 2.4678 - learning_rate: 0.0010\n",
      "Epoch 46/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9500 - loss: 1.9111 - val_accuracy: 0.5000 - val_loss: 2.4177 - learning_rate: 0.0010\n",
      "Epoch 47/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.7266 - val_accuracy: 0.5000 - val_loss: 2.3759 - learning_rate: 0.0010\n",
      "Epoch 48/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9667 - loss: 1.8561 - val_accuracy: 0.5000 - val_loss: 2.3283 - learning_rate: 0.0010\n",
      "Epoch 49/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9458 - loss: 1.7586 - val_accuracy: 0.5000 - val_loss: 2.2732 - learning_rate: 0.0010\n",
      "Epoch 50/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9833 - loss: 1.7116 - val_accuracy: 0.5000 - val_loss: 2.2254 - learning_rate: 0.0010\n",
      "Epoch 51/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.6607 - val_accuracy: 0.5000 - val_loss: 2.1853 - learning_rate: 0.0010\n",
      "Epoch 52/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.6434 - val_accuracy: 0.5000 - val_loss: 2.1528 - learning_rate: 0.0010\n",
      "Epoch 53/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.6305 - val_accuracy: 0.5000 - val_loss: 2.1258 - learning_rate: 0.0010\n",
      "Epoch 54/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9125 - loss: 1.7358 - val_accuracy: 0.7000 - val_loss: 2.0849 - learning_rate: 0.0010\n",
      "Epoch 55/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.5975 - val_accuracy: 0.7000 - val_loss: 2.0405 - learning_rate: 0.0010\n",
      "Epoch 56/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.6118 - val_accuracy: 0.7000 - val_loss: 2.0025 - learning_rate: 0.0010\n",
      "Epoch 57/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.5711 - val_accuracy: 0.8000 - val_loss: 1.9600 - learning_rate: 0.0010\n",
      "Epoch 58/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.5480 - val_accuracy: 0.8000 - val_loss: 1.9204 - learning_rate: 0.0010\n",
      "Epoch 59/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.5278 - val_accuracy: 0.8000 - val_loss: 1.8816 - learning_rate: 0.0010\n",
      "Epoch 60/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.5136 - val_accuracy: 0.8000 - val_loss: 1.8462 - learning_rate: 0.0010\n",
      "Epoch 61/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.4843 - val_accuracy: 0.8000 - val_loss: 1.8107 - learning_rate: 0.0010\n",
      "Epoch 62/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.4601 - val_accuracy: 1.0000 - val_loss: 1.7739 - learning_rate: 0.0010\n",
      "Epoch 63/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.4372 - val_accuracy: 1.0000 - val_loss: 1.7420 - learning_rate: 0.0010\n",
      "Epoch 64/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9833 - loss: 1.4582 - val_accuracy: 1.0000 - val_loss: 1.7163 - learning_rate: 0.0010\n",
      "Epoch 65/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.3872 - val_accuracy: 1.0000 - val_loss: 1.6976 - learning_rate: 0.0010\n",
      "Epoch 66/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.3681 - val_accuracy: 1.0000 - val_loss: 1.6816 - learning_rate: 0.0010\n",
      "Epoch 67/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.3494 - val_accuracy: 1.0000 - val_loss: 1.6614 - learning_rate: 0.0010\n",
      "Epoch 68/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9833 - loss: 1.3470 - val_accuracy: 1.0000 - val_loss: 1.6433 - learning_rate: 0.0010\n",
      "Epoch 69/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9833 - loss: 1.3121 - val_accuracy: 0.8000 - val_loss: 1.6354 - learning_rate: 0.0010\n",
      "Epoch 70/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 1.2787 - val_accuracy: 0.8000 - val_loss: 1.6308 - learning_rate: 0.0010\n",
      "Epoch 71/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.2603 - val_accuracy: 0.8000 - val_loss: 1.6236 - learning_rate: 0.0010\n",
      "Epoch 72/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.2448 - val_accuracy: 0.8000 - val_loss: 1.6106 - learning_rate: 0.0010\n",
      "Epoch 73/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.2246 - val_accuracy: 0.8000 - val_loss: 1.5923 - learning_rate: 0.0010\n",
      "Epoch 74/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9667 - loss: 1.2594 - val_accuracy: 0.9000 - val_loss: 1.5400 - learning_rate: 0.0010\n",
      "Epoch 75/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.1871 - val_accuracy: 1.0000 - val_loss: 1.4767 - learning_rate: 0.0010\n",
      "Epoch 76/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.1733 - val_accuracy: 1.0000 - val_loss: 1.4263 - learning_rate: 0.0010\n",
      "Epoch 77/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.1551 - val_accuracy: 1.0000 - val_loss: 1.3921 - learning_rate: 0.0010\n",
      "Epoch 78/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 1.1497 - val_accuracy: 1.0000 - val_loss: 1.3669 - learning_rate: 0.0010\n",
      "Epoch 79/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 1.1274 - val_accuracy: 1.0000 - val_loss: 1.3519 - learning_rate: 0.0010\n",
      "Epoch 80/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.1135 - val_accuracy: 1.0000 - val_loss: 1.3352 - learning_rate: 0.0010\n",
      "Epoch 81/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.0995 - val_accuracy: 1.0000 - val_loss: 1.3177 - learning_rate: 0.0010\n",
      "Epoch 82/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 1.0841 - val_accuracy: 1.0000 - val_loss: 1.3017 - learning_rate: 0.0010\n",
      "Epoch 83/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 1.0698 - val_accuracy: 1.0000 - val_loss: 1.2843 - learning_rate: 0.0010\n",
      "Epoch 84/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.0614 - val_accuracy: 1.0000 - val_loss: 1.2674 - learning_rate: 0.0010\n",
      "Epoch 85/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.0374 - val_accuracy: 1.0000 - val_loss: 1.2538 - learning_rate: 0.0010\n",
      "Epoch 86/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.0216 - val_accuracy: 1.0000 - val_loss: 1.2373 - learning_rate: 0.0010\n",
      "Epoch 87/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.0058 - val_accuracy: 1.0000 - val_loss: 1.2239 - learning_rate: 0.0010\n",
      "Epoch 88/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.9901 - val_accuracy: 1.0000 - val_loss: 1.2091 - learning_rate: 0.0010\n",
      "Epoch 89/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.9744 - val_accuracy: 1.0000 - val_loss: 1.1948 - learning_rate: 0.0010\n",
      "Epoch 90/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.9587 - val_accuracy: 1.0000 - val_loss: 1.1793 - learning_rate: 0.0010\n",
      "Epoch 91/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.9432 - val_accuracy: 1.0000 - val_loss: 1.1622 - learning_rate: 0.0010\n",
      "Epoch 92/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.9280 - val_accuracy: 1.0000 - val_loss: 1.1467 - learning_rate: 0.0010\n",
      "Epoch 93/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.9128 - val_accuracy: 1.0000 - val_loss: 1.1325 - learning_rate: 0.0010\n",
      "Epoch 94/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.8979 - val_accuracy: 1.0000 - val_loss: 1.1171 - learning_rate: 0.0010\n",
      "Epoch 95/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9833 - loss: 0.8958 - val_accuracy: 1.0000 - val_loss: 1.0884 - learning_rate: 0.0010\n",
      "Epoch 96/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.8689 - val_accuracy: 1.0000 - val_loss: 1.0578 - learning_rate: 0.0010\n",
      "Epoch 97/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.8556 - val_accuracy: 1.0000 - val_loss: 1.0355 - learning_rate: 0.0010\n",
      "Epoch 98/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.8430 - val_accuracy: 1.0000 - val_loss: 1.0176 - learning_rate: 0.0010\n",
      "Epoch 99/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.8308 - val_accuracy: 1.0000 - val_loss: 1.0017 - learning_rate: 0.0010\n",
      "Epoch 100/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.8192 - val_accuracy: 1.0000 - val_loss: 0.9884 - learning_rate: 0.0010\n",
      "Epoch 101/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.8069 - val_accuracy: 1.0000 - val_loss: 0.9748 - learning_rate: 0.0010\n",
      "Epoch 102/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7952 - val_accuracy: 1.0000 - val_loss: 0.9629 - learning_rate: 0.0010\n",
      "Epoch 103/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.7929 - val_accuracy: 1.0000 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 104/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.7719 - val_accuracy: 1.0000 - val_loss: 0.9221 - learning_rate: 0.0010\n",
      "Epoch 105/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9833 - loss: 0.9119 - val_accuracy: 1.0000 - val_loss: 0.9248 - learning_rate: 0.0010\n",
      "Epoch 106/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.7506 - val_accuracy: 1.0000 - val_loss: 0.9571 - learning_rate: 0.0010\n",
      "Epoch 107/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.7437 - val_accuracy: 1.0000 - val_loss: 0.9872 - learning_rate: 0.0010\n",
      "Epoch 108/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7450 - val_accuracy: 1.0000 - val_loss: 1.0094 - learning_rate: 0.0010\n",
      "Epoch 109/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9833 - loss: 0.7720 - val_accuracy: 1.0000 - val_loss: 1.0041 - learning_rate: 0.0010\n",
      "Epoch 110/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.7266 - val_accuracy: 1.0000 - val_loss: 0.9829 - learning_rate: 0.0010\n",
      "Epoch 111/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.7213 - val_accuracy: 1.0000 - val_loss: 0.9650 - learning_rate: 0.0010\n",
      "Epoch 112/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7163 - val_accuracy: 1.0000 - val_loss: 0.9486 - learning_rate: 0.0010\n",
      "Epoch 113/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7112 - val_accuracy: 1.0000 - val_loss: 0.9330 - learning_rate: 0.0010\n",
      "Epoch 114/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.7117 - val_accuracy: 1.0000 - val_loss: 0.9111 - learning_rate: 0.0010\n",
      "Epoch 115/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.7076 - val_accuracy: 1.0000 - val_loss: 0.8798 - learning_rate: 0.0010\n",
      "Epoch 116/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.6922 - val_accuracy: 1.0000 - val_loss: 0.8513 - learning_rate: 0.0010\n",
      "Epoch 117/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9833 - loss: 0.7090 - val_accuracy: 1.0000 - val_loss: 0.8222 - learning_rate: 0.0010\n",
      "Epoch 118/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.6790 - val_accuracy: 1.0000 - val_loss: 0.8017 - learning_rate: 0.0010\n",
      "Epoch 119/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.6729 - val_accuracy: 1.0000 - val_loss: 0.7950 - learning_rate: 0.0010\n",
      "Epoch 120/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9167 - loss: 1.2264 - val_accuracy: 1.0000 - val_loss: 0.8262 - learning_rate: 0.0010\n",
      "Epoch 121/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.6645 - val_accuracy: 0.8000 - val_loss: 0.9157 - learning_rate: 0.0010\n",
      "Epoch 122/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.6685 - val_accuracy: 0.8000 - val_loss: 1.0135 - learning_rate: 0.0010\n",
      "Epoch 123/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9396 - loss: 0.7635 - val_accuracy: 0.7000 - val_loss: 1.0898 - learning_rate: 0.0010\n",
      "Epoch 124/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9458 - loss: 0.7443 - val_accuracy: 0.7000 - val_loss: 1.1207 - learning_rate: 0.0010\n",
      "Epoch 125/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9729 - loss: 0.7528 - val_accuracy: 0.8000 - val_loss: 1.1137 - learning_rate: 0.0010\n",
      "Epoch 126/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9833 - loss: 0.7224 - val_accuracy: 0.7000 - val_loss: 1.0872 - learning_rate: 0.0010\n",
      "Epoch 127/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9729 - loss: 0.7330 - val_accuracy: 0.8000 - val_loss: 1.0485 - learning_rate: 0.0010\n",
      "Epoch 128/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.7090 - val_accuracy: 0.9000 - val_loss: 1.0162 - learning_rate: 0.0010\n",
      "Epoch 129/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.7108 - val_accuracy: 0.9000 - val_loss: 0.9845 - learning_rate: 0.0010\n",
      "Epoch 130/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9833 - loss: 0.7277 - val_accuracy: 0.9000 - val_loss: 0.9728 - learning_rate: 2.0000e-04\n",
      "Epoch 131/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.7108 - val_accuracy: 0.9000 - val_loss: 0.9559 - learning_rate: 2.0000e-04\n",
      "Epoch 132/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7104 - val_accuracy: 1.0000 - val_loss: 0.9412 - learning_rate: 2.0000e-04\n",
      "Epoch 133/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.7148 - val_accuracy: 1.0000 - val_loss: 0.9294 - learning_rate: 2.0000e-04\n",
      "Epoch 134/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.7077 - val_accuracy: 1.0000 - val_loss: 0.9168 - learning_rate: 2.0000e-04\n",
      "Epoch 135/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.7061 - val_accuracy: 1.0000 - val_loss: 0.9068 - learning_rate: 2.0000e-04\n",
      "Epoch 136/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7188 - val_accuracy: 1.0000 - val_loss: 0.8925 - learning_rate: 2.0000e-04\n",
      "Epoch 137/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.7028 - val_accuracy: 1.0000 - val_loss: 0.8825 - learning_rate: 2.0000e-04\n",
      "Epoch 138/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.7039 - val_accuracy: 1.0000 - val_loss: 0.8739 - learning_rate: 2.0000e-04\n",
      "Epoch 139/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.6984 - val_accuracy: 1.0000 - val_loss: 0.8625 - learning_rate: 2.0000e-04\n",
      "\n",
      "Training completed. Validation accuracy: 100.00%\n",
      "\n",
      "==== Face Recognition System with FaceNet ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 438\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 430\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    428\u001b[0m     model, label_encoder \u001b[38;5;241m=\u001b[39m train_model(dataset_dir, facenet_model)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[43mrealtime_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacenet_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExiting program...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 336\u001b[0m, in \u001b[0;36mrealtime_recognition\u001b[1;34m(model, label_encoder, facenet_model)\u001b[0m\n\u001b[0;32m    332\u001b[0m face_detector \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_detection\u001b[38;5;241m.\u001b[39mFaceDetection(\n\u001b[0;32m    333\u001b[0m     model_selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m--> 336\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mtcnn import MTCNN\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "class StopOnValLoss(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < 0.7:\n",
    "            print(f\"\\nEpoch {epoch + 1}: Validation loss below 0.7 ({val_loss:.4f}), stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Initialize face detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Initialize MTCNN for registration\n",
    "mtcnn_detector = MTCNN()\n",
    "\n",
    "# FaceNet model\n",
    "def load_facenet():\n",
    "    # Load pre-trained FaceNet model (InceptionResNetV2)\n",
    "    base_model = InceptionResNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(160, 160, 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Freeze all layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Create embedding model\n",
    "    input_layer = Input(shape=(160, 160, 3))\n",
    "    x = Lambda(preprocess_input)(input_layer)\n",
    "    x = base_model(x)\n",
    "    embeddings = Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=embeddings)\n",
    "    return model\n",
    "\n",
    "def check_nim_exists(nim, dataset_dir):\n",
    "    \"\"\"Check if NIM folder already exists\"\"\"\n",
    "    nim_folder = os.path.join(dataset_dir, nim)\n",
    "    return os.path.exists(nim_folder)\n",
    "\n",
    "def extract_face_embeddings(image, facenet_model):\n",
    "    \"\"\"Extract face embeddings using FaceNet\"\"\"\n",
    "    # Resize and preprocess image\n",
    "    image = cv2.resize(image, (160, 160))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Get embedding\n",
    "    embedding = facenet_model.predict(image, verbose=0)\n",
    "    return embedding[0]  # Return 1D embedding vector\n",
    "\n",
    "def load_dataset(dataset_dir, facenet_model):\n",
    "    \"\"\"Load dataset with FaceNet embeddings from all users\"\"\"\n",
    "    X, y = [], []\n",
    "    print(f\"\\nLoading dataset from {dataset_dir}\")\n",
    "    \n",
    "    # Get all NIM folders\n",
    "    nim_folders = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]\n",
    "    \n",
    "    if not nim_folders:\n",
    "        print(\"No registered users found in dataset!\")\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    for nim in nim_folders:\n",
    "        nim_path = os.path.join(dataset_dir, nim)\n",
    "        image_path = os.path.join(nim_path, \"image\")\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing NIM: {nim}\")\n",
    "        image_files = [f for f in os.listdir(image_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for img_name in image_files:\n",
    "            img_path = os.path.join(image_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            if image is None:\n",
    "                continue\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            embedding = extract_face_embeddings(image_rgb, facenet_model)\n",
    "            \n",
    "            if embedding is not None:\n",
    "                X.append(embedding)\n",
    "                y.append(nim)\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"Total samples loaded: {len(X)}\")\n",
    "    print(f\"Total unique NIMs: {len(set(y))}\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def register_new_user(dataset_dir, facenet_model):\n",
    "    \"\"\"Register new user with automatic capture for all angles (5 images each) with user prompt between angles\"\"\"\n",
    "    while True:\n",
    "        nim = input(\"\\nEnter NIM to register (or 'q' to quit): \").strip()\n",
    "        if nim.lower() == 'q':\n",
    "            return False, None, None\n",
    "            \n",
    "        if not nim.isdigit():\n",
    "            print(\"NIM must contain only numbers!\")\n",
    "            continue\n",
    "            \n",
    "        if check_nim_exists(nim, dataset_dir):\n",
    "            print(f\"\\nNIM {nim} already exists!\")\n",
    "            print(\"1. Register different NIM\")\n",
    "            print(\"2. Add more images to existing NIM\")\n",
    "            choice = input(\"Choose option (1/2): \")\n",
    "            \n",
    "            if choice == '1':\n",
    "                continue\n",
    "            elif choice == '2':\n",
    "                print(f\"\\nAdding more images to NIM {nim}\")\n",
    "            else:\n",
    "                print(\"Invalid choice!\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"\\nRegistering new NIM: {nim}\")\n",
    "            \n",
    "        # Create folder structure\n",
    "        nim_path = os.path.join(dataset_dir, nim)\n",
    "        image_path = os.path.join(nim_path, \"image\")\n",
    "        os.makedirs(image_path, exist_ok=True)\n",
    "        \n",
    "        # Capture images for different angles (5 images each)\n",
    "        angles = [\n",
    "            (\"front\", \"Please face the camera directly (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"up\", \"Please look upwards (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"down\", \"Please look downwards (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"left\", \"Please turn your head to the LEFT (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"right\", \"Please turn your head to the RIGHT (5 images will be captured automatically) - Press SPACE when ready\")\n",
    "        ]\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        captured_images = 0\n",
    "        \n",
    "        for angle_name, instruction in angles:\n",
    "            print(f\"\\n{instruction}\")\n",
    "            \n",
    "            # Wait for user to be ready (SPACE key)\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                # Display instruction\n",
    "                cv2.putText(frame, instruction, (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Press SPACE to start capture, ESC to skip\", (10, 60), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                cv2.imshow('Registration', frame)\n",
    "                \n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27:  # ESC to skip this angle\n",
    "                    print(f\"Skipping {angle_name} angle\")\n",
    "                    break\n",
    "                elif key == 32:  # SPACE to start capturing\n",
    "                    print(f\"Capturing 5 {angle_name} images automatically...\")\n",
    "                    \n",
    "                    # Automatic capture for this angle (5 images)\n",
    "                    for i in range(5):\n",
    "                        time.sleep(0.5)  # Reduced wait time between captures\n",
    "                        \n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                            \n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        results = mtcnn_detector.detect_faces(frame_rgb)\n",
    "                        \n",
    "                        if len(results) > 0:\n",
    "                            x, y, w, h = results[0]['box']\n",
    "                            # Expand bounding box\n",
    "                            x = max(0, x - int(w * 0.2))\n",
    "                            y = max(0, y - int(h * 0.2))\n",
    "                            w = min(frame.shape[1] - x, int(w * 1.4))\n",
    "                            h = min(frame.shape[0] - y, int(h * 1.4))\n",
    "                            \n",
    "                            face = frame_rgb[y:y+h, x:x+w]\n",
    "                            face_resized = cv2.resize(face, (160, 160))\n",
    "                            \n",
    "                            # Save image with timestamp to ensure unique filenames\n",
    "                            timestamp = int(time.time() * 1000)\n",
    "                            img_path = os.path.join(image_path, f\"{nim}_{angle_name}_{timestamp}_{i}.jpg\")\n",
    "                            cv2.imwrite(img_path, cv2.cvtColor(face_resized, cv2.COLOR_RGB2BGR))\n",
    "                            captured_images += 1\n",
    "                            \n",
    "                            # Show countdown and feedback\n",
    "                            cv2.putText(frame, f\"Captured {i+1}/5\", (x, y-10), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                            cv2.imshow('Registration', frame)\n",
    "                            cv2.waitKey(300)  # Reduced display time\n",
    "                        else:\n",
    "                            print(f\"No face detected for {angle_name} image {i+1}\")\n",
    "                            continue  # Skip this capture but continue trying\n",
    "                    \n",
    "                    print(f\"Finished capturing {angle_name} angle\")\n",
    "                    break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"\\nRegistration completed for NIM: {nim}\")\n",
    "        print(f\"Captured {captured_images} images in total\")\n",
    "        \n",
    "        # Verify we captured exactly 25 images (5 angles × 5 images)\n",
    "        if captured_images < 25:\n",
    "            print(f\"Warning: Only captured {captured_images} images (expected 25)\")\n",
    "        \n",
    "        # Train model after registration\n",
    "        print(\"\\nStarting automatic model training with updated dataset...\")\n",
    "        model, label_encoder = train_model(dataset_dir, facenet_model)\n",
    "        \n",
    "        return True, model, label_encoder\n",
    "\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    \"\"\"Build the classifier model on top of FaceNet embeddings\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.005)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(dataset_dir, facenet_model):\n",
    "    \"\"\"Train the classifier model with all available data\"\"\"\n",
    "    X, y = load_dataset(dataset_dir, facenet_model)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"\\nNo training data available!\")\n",
    "        return None, None\n",
    "        \n",
    "    # Encode labels (NIMs)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    \n",
    "    # Split dataset (80% train, 20% validation)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_categorical, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "    \n",
    "    # Calculate class weights to handle imbalanced data\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Build classifier model\n",
    "    model = build_classifier_model((X.shape[1],), len(label_encoder.classes_))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10),\n",
    "        ModelCheckpoint('best_facenet_model.keras', monitor='val_accuracy', save_best_only=True),\n",
    "        StopOnValLoss()\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining classifier model with FaceNet embeddings...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"\\nTraining completed. Validation accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    np.save('facenet_label_encoder.npy', label_encoder.classes_)\n",
    "    \n",
    "    return model, label_encoder\n",
    "\n",
    "def realtime_recognition(model, label_encoder, facenet_model):\n",
    "    \"\"\"Real-time recognition for all registered users using FaceNet\"\"\"\n",
    "    if model is None or label_encoder is None:\n",
    "        print(\"Model not trained yet! Please train first.\")\n",
    "        return\n",
    "        \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    label_map = {i:name for i,name in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    # Load face detection model\n",
    "    face_detector = mp.solutions.face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = face_detector.process(frame_rgb)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get bounding box\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                             int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                # Extract face ROI\n",
    "                face_roi = frame_rgb[y:y+h, x:x+w]\n",
    "                \n",
    "                try:\n",
    "                    # Resize face for FaceNet\n",
    "                    face_resized = cv2.resize(face_roi, (160, 160))\n",
    "                    \n",
    "                    # Get FaceNet embedding\n",
    "                    embedding = extract_face_embeddings(face_resized, facenet_model)\n",
    "                    \n",
    "                    if embedding is not None:\n",
    "                        # Predict using classifier\n",
    "                        predictions = model.predict(embedding.reshape(1, -1), verbose=0)\n",
    "                        idx = np.argmax(predictions)\n",
    "                        confidence = np.max(predictions)\n",
    "                        \n",
    "                        # Get NIM label\n",
    "                        nim = label_map[idx]\n",
    "                        \n",
    "                        # Draw results\n",
    "                        color = (0, 255, 0) if confidence > 0.7 else (0, 255, 255) if confidence > 0.5 else (0, 0, 255)\n",
    "                        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                        cv2.putText(frame, f\"NIM: {nim} ({confidence*100:.1f}%)\", \n",
    "                                   (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing face: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        cv2.imshow('Face Recognition - All Registered Users', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    dataset_dir = 'dataset'  # Folder structure: /dataset/NIM/image/\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize FaceNet model\n",
    "    facenet_model = load_facenet()\n",
    "    \n",
    "    # Initialize classifier model and label encoder\n",
    "    model = None\n",
    "    label_encoder = None\n",
    "    \n",
    "    # Try to load existing model and label encoder\n",
    "    try:\n",
    "        model = load_model('best_facenet_model.keras')\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.classes_ = np.load('facenet_label_encoder.npy', allow_pickle=True)\n",
    "        print(\"\\nLoaded existing trained model with\", len(label_encoder.classes_), \"registered users\")\n",
    "    except:\n",
    "        print(\"\\nNo existing model found or error loading model\")\n",
    "        \n",
    "        # If dataset exists but no model, train new model\n",
    "        if len(os.listdir(dataset_dir)) > 0:\n",
    "            print(\"Found existing dataset, training new model...\")\n",
    "            model, label_encoder = train_model(dataset_dir, facenet_model)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n==== Face Recognition System with FaceNet ====\")\n",
    "        print(\"1. Register new user\")\n",
    "        print(\"2. Train/retrain model with all data\")\n",
    "        print(\"3. Real-time recognition (all users)\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"Select option: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            success, new_model, new_label_encoder = register_new_user(dataset_dir, facenet_model)\n",
    "            if success:\n",
    "                model = new_model\n",
    "                label_encoder = new_label_encoder\n",
    "        elif choice == '2':\n",
    "            model, label_encoder = train_model(dataset_dir, facenet_model)\n",
    "        elif choice == '3':\n",
    "            realtime_recognition(model, label_encoder, facenet_model)\n",
    "        elif choice == '4':\n",
    "            print(\"Exiting program...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
