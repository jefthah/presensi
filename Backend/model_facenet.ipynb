{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f692175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No existing model found or error loading model\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n",
      "Invalid choice!\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n",
      "Invalid choice!\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n",
      "Invalid choice!\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n",
      "Invalid choice!\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n",
      "Invalid choice!\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n",
      "Invalid choice!\n",
      "\n",
      "==== Face Recognition System ====\n",
      "1. Register new user\n",
      "2. Train/retrain model with all data\n",
      "3. Real-time recognition (all users)\n",
      "4. Exit\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mtcnn import MTCNN\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class StopOnValLoss(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if val_loss is not None and val_loss < 0.7:\n",
    "            print(f\"\\nEpoch {epoch + 1}: Validation loss below 0.7 ({val_loss:.4f}), stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Initialize face detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Initialize MTCNN for registration\n",
    "mtcnn_detector = MTCNN()\n",
    "\n",
    "# Selected important facial landmarks (indices from MediaPipe's 468 landmarks)\n",
    "IMPORTANT_LANDMARKS = [\n",
    "    10,   # Forehead\n",
    "    152,  # Chin\n",
    "    234,  # Left cheek\n",
    "    454,  # Right cheek\n",
    "    168,  # Nose tip\n",
    "    33,   # Left eyebrow\n",
    "    263,  # Right eyebrow\n",
    "    61,   # Left eye\n",
    "    291,  # Right eye\n",
    "    0,    # Nose bridge\n",
    "    17,   # Upper lip\n",
    "    57,   # Mouth left corner\n",
    "    287,  # Mouth right corner\n",
    "]\n",
    "\n",
    "def check_nim_exists(nim, dataset_dir):\n",
    "    \"\"\"Check if NIM folder already exists\"\"\"\n",
    "    nim_folder = os.path.join(dataset_dir, nim)\n",
    "    return os.path.exists(nim_folder)\n",
    "\n",
    "def extract_mediapipe_landmarks(image):\n",
    "    \"\"\"Extract and customize facial landmarks with distance features\"\"\"\n",
    "    results = face_mesh.process(image)\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        \n",
    "        # Get selected landmarks\n",
    "        selected_landmarks = []\n",
    "        for idx in IMPORTANT_LANDMARKS:\n",
    "            landmark = face_landmarks.landmark[idx]\n",
    "            selected_landmarks.append([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        # Calculate center point\n",
    "        center_x = sum(lm[0] for lm in selected_landmarks)/len(selected_landmarks)\n",
    "        center_y = sum(lm[1] for lm in selected_landmarks)/len(selected_landmarks)\n",
    "        center_z = sum(lm[2] for lm in selected_landmarks)/len(selected_landmarks)\n",
    "        \n",
    "        # Enhanced features\n",
    "        features = []\n",
    "        \n",
    "        # 1. Normalized coordinates relative to center\n",
    "        for lm in selected_landmarks:\n",
    "            features.extend([\n",
    "                lm[0] - center_x,  # X coordinate relative to center\n",
    "                lm[1] - center_y,  # Y coordinate relative to center\n",
    "                lm[2] - center_z   # Z coordinate relative to center\n",
    "            ])\n",
    "        \n",
    "        # 2. Distance between key facial points\n",
    "        def calc_distance(idx1, idx2):\n",
    "            lm1 = selected_landmarks[idx1]\n",
    "            lm2 = selected_landmarks[idx2]\n",
    "            return ((lm1[0]-lm2[0])**2 + (lm1[1]-lm2[1])**2 + (lm1[2]-lm2[2])**2)**0.5\n",
    "        \n",
    "        # Add important distances\n",
    "        features.extend([\n",
    "            calc_distance(0, 1),   # Forehead to chin\n",
    "            calc_distance(3, 4),   # Left cheek to right cheek\n",
    "            calc_distance(5, 6),   # Left eyebrow to right eyebrow\n",
    "            calc_distance(7, 8),   # Left eye to right eye\n",
    "            calc_distance(9, 10),  # Nose bridge to nose tip\n",
    "            calc_distance(11, 12)  # Mouth left to right corner\n",
    "        ])\n",
    "        \n",
    "        # 3. Facial ratios (important for distinguishing faces)\n",
    "        features.extend([\n",
    "            calc_distance(0, 1) / calc_distance(3, 4),  # Face height/width ratio\n",
    "            calc_distance(5, 6) / calc_distance(7, 8),  # Eyebrow/eye width ratio\n",
    "            calc_distance(9, 10) / calc_distance(0, 1)  # Nose length/face height\n",
    "        ])\n",
    "        \n",
    "        # 4. Angle features\n",
    "        def calc_angle(idx1, idx2, idx3):\n",
    "            a = np.array(selected_landmarks[idx1])\n",
    "            b = np.array(selected_landmarks[idx2])\n",
    "            c = np.array(selected_landmarks[idx3])\n",
    "            \n",
    "            ba = a - b\n",
    "            bc = c - b\n",
    "            \n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "            return np.arccos(cosine_angle)\n",
    "        \n",
    "        features.extend([\n",
    "            calc_angle(0, 9, 1),   # Forehead-nose-chin angle\n",
    "            calc_angle(3, 4, 2),   # Cheek-nose-cheek angle\n",
    "            calc_angle(5, 7, 6)    # Eyebrow-eye-eyebrow angle\n",
    "        ])\n",
    "        \n",
    "        return np.array(features)\n",
    "    return None\n",
    "\n",
    "def load_dataset(dataset_dir):\n",
    "    \"\"\"Load dataset with enhanced landmark features from all users\"\"\"\n",
    "    X, y = [], []\n",
    "    print(f\"\\nLoading dataset from {dataset_dir}\")\n",
    "    \n",
    "    # Get all NIM folders\n",
    "    nim_folders = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]\n",
    "    \n",
    "    if not nim_folders:\n",
    "        print(\"No registered users found in dataset!\")\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    for nim in nim_folders:\n",
    "        nim_path = os.path.join(dataset_dir, nim)\n",
    "        image_path = os.path.join(nim_path, \"image\")\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing NIM: {nim}\")\n",
    "        image_files = [f for f in os.listdir(image_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for img_name in image_files:\n",
    "            img_path = os.path.join(image_path, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            if image is None:\n",
    "                continue\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            landmarks = extract_mediapipe_landmarks(image_rgb)\n",
    "            \n",
    "            if landmarks is not None:\n",
    "                X.append(landmarks)\n",
    "                y.append(nim)\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"Total samples loaded: {len(X)}\")\n",
    "    print(f\"Total unique NIMs: {len(set(y))}\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def register_new_user(dataset_dir):\n",
    "    \"\"\"Register new user with automatic capture for all angles (5 images each) with user prompt between angles\"\"\"\n",
    "    while True:\n",
    "        nim = input(\"\\nEnter NIM to register (or 'q' to quit): \").strip()\n",
    "        if nim.lower() == 'q':\n",
    "            return False, None, None\n",
    "            \n",
    "        if not nim.isdigit():\n",
    "            print(\"NIM must contain only numbers!\")\n",
    "            continue\n",
    "            \n",
    "        if check_nim_exists(nim, dataset_dir):\n",
    "            print(f\"\\nNIM {nim} already exists!\")\n",
    "            print(\"1. Register different NIM\")\n",
    "            print(\"2. Add more images to existing NIM\")\n",
    "            choice = input(\"Choose option (1/2): \")\n",
    "            \n",
    "            if choice == '1':\n",
    "                continue\n",
    "            elif choice == '2':\n",
    "                print(f\"\\nAdding more images to NIM {nim}\")\n",
    "            else:\n",
    "                print(\"Invalid choice!\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"\\nRegistering new NIM: {nim}\")\n",
    "            \n",
    "        # Create folder structure\n",
    "        nim_path = os.path.join(dataset_dir, nim)\n",
    "        image_path = os.path.join(nim_path, \"image\")\n",
    "        os.makedirs(image_path, exist_ok=True)\n",
    "        \n",
    "        # Capture images for different angles (5 images each)\n",
    "        angles = [\n",
    "            (\"front\", \"Please face the camera directly (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"up\", \"Please look upwards (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"down\", \"Please look downwards (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"left\", \"Please turn your head to the LEFT (5 images will be captured automatically) - Press SPACE when ready\"),\n",
    "            (\"right\", \"Please turn your head to the RIGHT (5 images will be captured automatically) - Press SPACE when ready\")\n",
    "        ]\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        captured_images = 0\n",
    "        \n",
    "        for angle_name, instruction in angles:\n",
    "            print(f\"\\n{instruction}\")\n",
    "            \n",
    "            # Wait for user to be ready (SPACE key)\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                # Display instruction\n",
    "                cv2.putText(frame, instruction, (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Press SPACE to start capture, ESC to skip\", (10, 60), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                cv2.imshow('Registration', frame)\n",
    "                \n",
    "                key = cv2.waitKey(1)\n",
    "                if key == 27:  # ESC to skip this angle\n",
    "                    print(f\"Skipping {angle_name} angle\")\n",
    "                    break\n",
    "                elif key == 32:  # SPACE to start capturing\n",
    "                    print(f\"Capturing 5 {angle_name} images automatically...\")\n",
    "                    \n",
    "                    # Automatic capture for this angle (5 images)\n",
    "                    for i in range(5):\n",
    "                        time.sleep(0.5)  # Reduced wait time between captures\n",
    "                        \n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                            \n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        results = mtcnn_detector.detect_faces(frame_rgb)\n",
    "                        \n",
    "                        if len(results) > 0:\n",
    "                            x, y, w, h = results[0]['box']\n",
    "                            # Expand bounding box\n",
    "                            x = max(0, x - int(w * 0.2))\n",
    "                            y = max(0, y - int(h * 0.2))\n",
    "                            w = min(frame.shape[1] - x, int(w * 1.4))\n",
    "                            h = min(frame.shape[0] - y, int(h * 1.4))\n",
    "                            \n",
    "                            face = frame_rgb[y:y+h, x:x+w]\n",
    "                            face_resized = cv2.resize(face, (160, 160))\n",
    "                            \n",
    "                            # Save image with timestamp to ensure unique filenames\n",
    "                            timestamp = int(time.time() * 1000)\n",
    "                            img_path = os.path.join(image_path, f\"{nim}_{angle_name}_{timestamp}_{i}.jpg\")\n",
    "                            cv2.imwrite(img_path, cv2.cvtColor(face_resized, cv2.COLOR_RGB2BGR))\n",
    "                            captured_images += 1\n",
    "                            \n",
    "                            # Show countdown and feedback\n",
    "                            cv2.putText(frame, f\"Captured {i+1}/5\", (x, y-10), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                            cv2.imshow('Registration', frame)\n",
    "                            cv2.waitKey(300)  # Reduced display time\n",
    "                        else:\n",
    "                            print(f\"No face detected for {angle_name} image {i+1}\")\n",
    "                            continue  # Skip this capture but continue trying\n",
    "                    \n",
    "                    print(f\"Finished capturing {angle_name} angle\")\n",
    "                    break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"\\nRegistration completed for NIM: {nim}\")\n",
    "        print(f\"Captured {captured_images} images in total\")\n",
    "        \n",
    "        # Verify we captured exactly 25 images (5 angles × 5 images)\n",
    "        if captured_images < 25:\n",
    "            print(f\"Warning: Only captured {captured_images} images (expected 25)\")\n",
    "        \n",
    "        # Train model after registration\n",
    "        print(\"\\nStarting automatic model training with updated dataset...\")\n",
    "        model, label_encoder = train_model(dataset_dir)\n",
    "        \n",
    "        return True, model, label_encoder\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build the recognition model with enhanced features\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.005)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(dataset_dir):\n",
    "    \"\"\"Train the model with all available data\"\"\"\n",
    "    X, y = load_dataset(dataset_dir)\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"\\nNo training data available!\")\n",
    "        return None, None\n",
    "        \n",
    "    # Encode labels (NIMs)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    \n",
    "    # Split dataset (80% train, 20% validation)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_categorical, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "    \n",
    "    # Calculate class weights to handle imbalanced data\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model((X.shape[1],), len(label_encoder.classes_))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10),\n",
    "        ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True),\n",
    "        StopOnValLoss()\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining model with all available data...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"\\nTraining completed. Validation accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    np.save('label_encoder.npy', label_encoder.classes_)\n",
    "    \n",
    "    return model, label_encoder\n",
    "\n",
    "def realtime_recognition(model, label_encoder):\n",
    "    \"\"\"Real-time recognition for all registered users\"\"\"\n",
    "    if model is None or label_encoder is None:\n",
    "        print(\"Model not trained yet! Please train first.\")\n",
    "        return\n",
    "        \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    label_map = {i:name for i,name in enumerate(label_encoder.classes_)}\n",
    "    \n",
    "    # Load face detection model\n",
    "    face_detector = mp.solutions.face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = face_detector.process(frame_rgb)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get bounding box\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                             int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                # Extract face ROI\n",
    "                face_roi = frame_rgb[y:y+h, x:x+w]\n",
    "                \n",
    "                # Extract landmarks and predict\n",
    "                landmarks = extract_mediapipe_landmarks(face_roi)\n",
    "                \n",
    "                if landmarks is not None:\n",
    "                    features = landmarks.reshape(1, -1)\n",
    "                    predictions = model.predict(features, verbose=0)\n",
    "                    idx = np.argmax(predictions)\n",
    "                    confidence = np.max(predictions)\n",
    "                    \n",
    "                    # Get NIM label\n",
    "                    nim = label_map[idx]\n",
    "                    \n",
    "                    # Draw results\n",
    "                    color = (0, 255, 0) if confidence > 0.7 else (0, 255, 255) if confidence > 0.5 else (0, 0, 255)\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                    cv2.putText(frame, f\"NIM: {nim} ({confidence*100:.1f}%)\", \n",
    "                               (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        cv2.imshow('Face Recognition - All Registered Users', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def extract_face_and_landmarks(image_rgb):\n",
    "    \"\"\"Extract face and enhanced landmarks for real-time detection\"\"\"\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        # Get face bounding box\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        xs = [lm.x for lm in landmarks]\n",
    "        ys = [lm.y for lm in landmarks]\n",
    "        \n",
    "        x_min, x_max = min(xs), max(xs)\n",
    "        y_min, y_max = min(ys), max(ys)\n",
    "        \n",
    "        h, w, _ = image_rgb.shape\n",
    "        x1, y1 = int(x_min * w), int(y_min * h)\n",
    "        x2, y2 = int(x_max * w), int(y_max * h)\n",
    "        \n",
    "        # Extract enhanced landmarks\n",
    "        landmark_features = extract_mediapipe_landmarks(image_rgb)\n",
    "        \n",
    "        return landmark_features, (x1, y1, x2-x1, y2-y1)\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def main():\n",
    "    dataset_dir = 'dataset'  # Folder structure: /dataset/NIM/image/\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize model and label encoder\n",
    "    model = None\n",
    "    label_encoder = None\n",
    "    \n",
    "    # Try to load existing model and label encoder\n",
    "    try:\n",
    "        model = load_model('best_model.keras')\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.classes_ = np.load('label_encoder.npy', allow_pickle=True)\n",
    "        print(\"\\nLoaded existing trained model with\", len(label_encoder.classes_), \"registered users\")\n",
    "    except:\n",
    "        print(\"\\nNo existing model found or error loading model\")\n",
    "        \n",
    "        # If dataset exists but no model, train new model\n",
    "        if len(os.listdir(dataset_dir)) > 0:\n",
    "            print(\"Found existing dataset, training new model...\")\n",
    "            model, label_encoder = train_model(dataset_dir)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n==== Face Recognition System ====\")\n",
    "        print(\"1. Register new user\")\n",
    "        print(\"2. Train/retrain model with all data\")\n",
    "        print(\"3. Real-time recognition (all users)\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"Select option: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            success, new_model, new_label_encoder = register_new_user(dataset_dir)\n",
    "            if success:\n",
    "                model = new_model\n",
    "                label_encoder = new_label_encoder\n",
    "        elif choice == '2':\n",
    "            model, label_encoder = train_model(dataset_dir)\n",
    "        elif choice == '3':\n",
    "            realtime_recognition(model, label_encoder)\n",
    "        elif choice == '4':\n",
    "            print(\"Exiting program...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
